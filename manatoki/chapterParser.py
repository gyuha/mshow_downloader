import os
import pathlib
import re
import shutil
import time

from bs4 import BeautifulSoup
from common import driver
from common.dataSave import loadJsonFile, saveJsonFile
from common.driver import reconnect, retry_wait
from common.imagesDownload import imagesDownload, pathName

from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

from manatoki.chapters import chapterListParser
from manatoki.config import Config

BASE_URL = '/comic/'
imageDownloadTryCount = 0


def saveFolderPath(titlePath, num):
    path = os.path.join(titlePath, "%03d" % (num,))
    return path


def comicsDownload(driver, mangaId, downloadFolder):
    # ë§Œí™”ì±…ì—ì„œ ì´ë¯¸ì§€ ëª©ë¡ì„ ê°€ì ¸ ì™€ì„œ ë‹¤ìš´ë¡œë“œ í•˜ê¸°
    chaterList, public_type, tags, author, title = chapterListParser(
        driver, mangaId)
    global imageDownloadTryCount

    if len(chaterList) == 0:
        print("[Error] ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íƒ€ì´í‹€ì„ í™•ì¸ í•´ ì£¼ì„¸ìš”.")
        return

    titlePath = os.path.join(downloadFolder, pathName(title))

    pathlib.Path(titlePath).mkdir(parents=True, exist_ok=True)
    os.utime(pathlib.Path(titlePath), None)
    skip_num = 0
    saveData = loadJsonFile(os.path.join(titlePath, "data.json"))
    if saveData:
        skip_num = int(saveData["skip"])

    num = 1
    for d in chaterList:
        c = Config()
        url = d["href"]
        if skip_num >= num:
            print("[" + str(num) + "/" + str(len(chaterList)) +
                  "] íŒ¨ìŠ¤ : " + d["title"], end="\r")
            num = num + 1
            continue
        savePath = saveFolderPath(titlePath, num)
        print(" "*80, end="\r")
        print("[" + str(num) + "/" + str(len(chaterList)) + "] ë‹¤ìš´ë¡œë“œ : " + d["title"])
        num = num + 1
        if os.path.exists(savePath + "." + c.getFileExtension()) or os.path.exists(savePath + ".zip"):
            print("  ì´ë¯¸ ì••ì¶•í•œ íŒŒì¼ :" + d["title"])
            continue
        print("  Get image list by url..", end="\r")

        imageDownloadTryCount = 0
        images, chapter, seed = getImageList(driver, url)
        print("  Download images..      ", end="\r")

        if len(images) == 0:
            print("  ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒ¨ìŠ¤")
            continue
        imagesDownload(d["title"], savePath, images, chapter, seed)

        # ìµœê·¼ ë°›ì€ íŒŒì¼ì„ JSONìœ¼ë¡œ ì €ì¥í•˜ê¸°
        json = {
            'author': author,
            'skip': num-1,
            'title': title,
            'public_type': public_type,
            'tags': tags,
            'id': mangaId
        }
        saveJsonFile(os.path.join(titlePath, "data.json"), json)

    # ì™„ê²°ì¸ ì±…ìë¥¼ ë³„ë„ë¡œ ì €ì¥í•´ ì¤€ë‹¤.
    if public_type == "ì™„ê²°" or public_type == "ë‹¨í¸":
        print("#"*80)
        print("#"*2 + " ì™„ê²°: " + title)
        print("#"*80)
        title = pathName(title)
        tar = os.path.join("complete", title)
        pathlib.Path(os.path.join("complete")).mkdir(
            parents=True, exist_ok=True)
        if os.path.exists(tar):
            shutil.rmtree(tar, ignore_errors=True)
        shutil.move(titlePath, tar)
        shutil.rmtree(titlePath, ignore_errors=True)

    print("[*] Download Complete")


def parseImages(driver):
    time.sleep(0.5)
    view = driver.find_elements_by_class_name("view-padding")[1]

    imgs = view.find_elements_by_tag_name("img")

    for i in reversed(range(len(imgs))):
        if not imgs[i].is_displayed():
            # ë³´ì´ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ëŠ” ì œê±°
            del imgs[i]

    img_list = []

    # imageRe = re.compile("data-.*=\"(.*\.\w{3,4})\"")
    imageRe = re.compile(
        "data-.*=\"(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\"")
    for img in imgs:
        imgTag = img.get_attribute('outerHTML')
        if imageRe.search(imgTag) != None:
            img_list.append(imageRe.search(imgTag).group(1))

    print('ğŸ“¢[chapterParser.py:122]:', img_list)
    source = driver.page_source

    chapter = 0
    seed = 0

    # ì•„ë˜ ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ë¡œë”©ì´ ë˜ì§€ ì•Šì€ ê²ƒì„.
    if "ë·°ì–´ë¡œ ë³´ê¸°" not in source or len(img_list) == 0:
        return [], chapter, seed, False

    return img_list, chapter, seed, True


def getImageList(driver, url):
    global imageDownloadTryCount
    imageDownloadTryCount = imageDownloadTryCount + 1
    wait = WebDriverWait(driver, 30)
    try:
        driver.get(url)
        wait.until(EC.visibility_of_element_located(
            (By.CSS_SELECTOR, '.comic-navbar')))
        time.sleep(0.5)
        driver.execute_script("window.stop();")
    except Exception:
        # reconnect(driver)
        print('ì‚¬ì´íŠ¸ ì½ê¸° ì˜¤ë¥˜')
        if imageDownloadTryCount > 2:
            return [], 0, 0
        return getImageList(driver, url)

    contents, chapter, seed, loading = parseImages(driver)

    # ë¡œë”©ì´ ë˜ì§€ ì•Šì•˜ìœ¼ë©´... ë‹¤ì‹œ ì½ê¸°
    if loading == False:
        if imageDownloadTryCount > 2:
            return [], 0, 0
        retry_wait(7, "[ì´ë¯¸ì§€ëª©ë¡] ")
        contents, chapter, seed, loading = parseImages(driver)
        # ì‹œê°„ì´ ì§€ë‚¬ëŠ”ë°ë„ ë¡œë”©ì´ ë˜ì§€ ì•Šìœ¼ë©´..
        if loading == False:
            return getImageList(driver, url)

    # ë¡œë”©ì´ ë˜ì—ˆì§€ë§Œ, ë°ì´í„°ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
    if loading == True and len(contents) == 0:
        return [], 0, 0

    return contents, chapter, seed
